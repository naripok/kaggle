{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Training Baseline\n",
    "\n",
    "I wanted to create my own baseline for this competition, and I tried to do so \"without peeking\" at the kernels published by others. Ideally this can be used for training on a Kaggle kernel. Let's see how good we can get. \n",
    "\n",
    "This baseline is based on the following notebook by Sylvain Gugger: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "\n",
    "I initially started building with Roberta - thanks to Chris Deotte for pointing to Longformer :) The evaluation code is from Rob Mulla.\n",
    "\n",
    "The notebook requires a couple of hours to run, so we'll use W&B to be able to monitor it along the way and keep the record of our experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE = False # set True for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Requirement already satisfied: torch==1.10.2+cu113 in /opt/conda/lib/python3.9/site-packages (1.10.2+cu113)\n",
      "Requirement already satisfied: torchvision==0.11.3+cu113 in /opt/conda/lib/python3.9/site-packages (0.11.3+cu113)\n",
      "Requirement already satisfied: torchaudio==0.10.2+cu113 in /opt/conda/lib/python3.9/site-packages (0.10.2+cu113)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.10.2+cu113) (3.7.4.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision==0.11.3+cu113) (8.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchvision==0.11.3+cu113) (1.19.5)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.16.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (1.18.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval -qq # evaluation metrics for training (not the competition metric)\n",
    "!pip install --upgrade wandb -qq # experiment tracking\n",
    "!pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio==0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaripok\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/naripok/feedback_prize/runs/324yagu1\" target=\"_blank\">dauntless-moon-2</a></strong> to <a href=\"https://wandb.ai/naripok/feedback_prize\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import patch_path\n",
    "patch_path()\n",
    "\n",
    "# setup wandb for experiment tracking\n",
    "# source: https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter\n",
    "\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=api_key)\n",
    "    wandb.init(project=\"feedback_prize\", entity=\"naripok\")\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "EXP_NUM = 4\n",
    "task = \"ner\"\n",
    "model_checkpoint = \"allenai/longformer-base-4096\"\n",
    "max_length = 1024\n",
    "stride = 128\n",
    "min_tokens = 6\n",
    "model_path = f'{model_checkpoint.split(\"/\")[-1]}-{EXP_NUM}'\n",
    "\n",
    "# TRAINING HYPERPARAMS\n",
    "BS = 4\n",
    "GRAD_ACC = 8\n",
    "LR = 5e-5\n",
    "WD = 0.01\n",
    "WARMUP = 0.1\n",
    "N_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read train data\n",
    "train = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lead',\n",
       " 'Position',\n",
       " 'Evidence',\n",
       " 'Claim',\n",
       " 'Concluding Statement',\n",
       " 'Counterclaim',\n",
       " 'Rebuttal']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique classes\n",
    "classes = train.discourse_type.unique().tolist()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup label indices\n",
    "\n",
    "from collections import defaultdict\n",
    "tags = defaultdict()\n",
    "\n",
    "for i, c in enumerate(classes):\n",
    "    tags[f'B-{c}'] = i\n",
    "    tags[f'I-{c}'] = i + len(classes)\n",
    "tags[f'O'] = len(classes) * 2\n",
    "tags[f'Special'] = -100\n",
    "    \n",
    "l2i = dict(tags)\n",
    "\n",
    "i2l = defaultdict()\n",
    "for k, v in l2i.items(): \n",
    "    i2l[v] = k\n",
    "i2l[-100] = 'Special'\n",
    "\n",
    "i2l = dict(i2l)\n",
    "\n",
    "N_LABELS = len(i2l) - 1 # not accounting for -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('../input/feedback-prize-2021/train')\n",
    "\n",
    "def get_raw_text(ids):\n",
    "    with open(path/f'{ids}.txt', 'r') as file: data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>classlist</th>\n",
       "      <th>starts</th>\n",
       "      <th>ends</th>\n",
       "      <th>predictionstrings</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>[Position, Evidence, Evidence, Claim, Counterc...</td>\n",
       "      <td>[0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...</td>\n",
       "      <td>[170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...</td>\n",
       "      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...</td>\n",
       "      <td>[455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...</td>\n",
       "      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...</td>\n",
       "      <td>[56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....</td>\n",
       "      <td>[2 3 4 5 6 7 8, 10 11 12 13 14 15 16 17 18 19 ...</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>[Lead, Evidence, Claim, Claim, Evidence, Claim...</td>\n",
       "      <td>[0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...</td>\n",
       "      <td>[160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...</td>\n",
       "      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...</td>\n",
       "      <td>[57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...</td>\n",
       "      <td>[0 1 2 3 4 5 6 7 8 9, 10 11 12 13 14 15, 16 17...</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          classlist  \\\n",
       "0  0000D23A521A  [Position, Evidence, Evidence, Claim, Counterc...   \n",
       "1  00066EA9880D  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "2  000E6DE9E817  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "3  001552828BD0  [Lead, Evidence, Claim, Claim, Evidence, Claim...   \n",
       "4  0016926B079C  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "\n",
       "                                              starts  \\\n",
       "0  [0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...   \n",
       "1  [0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...   \n",
       "2  [17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...   \n",
       "3  [0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...   \n",
       "4  [0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...   \n",
       "\n",
       "                                                ends  \\\n",
       "0  [170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...   \n",
       "1  [455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...   \n",
       "2  [56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....   \n",
       "3  [160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...   \n",
       "4  [57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...   \n",
       "\n",
       "                                   predictionstrings  \\\n",
       "0  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n",
       "1  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n",
       "2  [2 3 4 5 6 7 8, 10 11 12 13 14 15 16 17 18 19 ...   \n",
       "3  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n",
       "4  [0 1 2 3 4 5 6 7 8 9, 10 11 12 13 14 15, 16 17...   \n",
       "\n",
       "                                                text  \n",
       "0  Some people belive that the so called \"face\" o...  \n",
       "1  Driverless cars are exaclty what you would exp...  \n",
       "2  Dear: Principal\\n\\nI am arguing against the po...  \n",
       "3  Would you be able to give your car up? Having ...  \n",
       "4  I think that students would benefit from learn...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group training labels by text file\n",
    "\n",
    "df1 = train.groupby('id')['discourse_type'].apply(list).reset_index(name='classlist')\n",
    "df2 = train.groupby('id')['discourse_start'].apply(list).reset_index(name='starts')\n",
    "df3 = train.groupby('id')['discourse_end'].apply(list).reset_index(name='ends')\n",
    "df4 = train.groupby('id')['predictionstring'].apply(list).reset_index(name='predictionstrings')\n",
    "\n",
    "df = pd.merge(df1, df2, how='inner', on='id')\n",
    "df = pd.merge(df, df3, how='inner', on='id')\n",
    "df = pd.merge(df, df4, how='inner', on='id')\n",
    "df['text'] = df['id'].apply(get_raw_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# debugging\n",
    "if SAMPLE: df = df.sample(n=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'classlist', 'starts', 'ends', 'predictionstrings', 'text', '__index_level_0__'],\n",
       "        num_rows: 14034\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'classlist', 'starts', 'ends', 'predictionstrings', 'text', '__index_level_0__'],\n",
       "        num_rows: 1560\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use HuggingFace datasets\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "datasets = ds.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 7, 7, 1, 1, 8, 8, 8, 2, 9, 9, 14, 4, 4, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not sure if this is needed, but in case we create a span with certain class without starting token of that class,\n",
    "# let's convert the first token to be the starting token.\n",
    "\n",
    "e = [0,7,7,7,1,1,8,8,8,9,9,9,14,4,4,4]\n",
    "\n",
    "def fix_beginnings(labels):\n",
    "    for i in range(1,len(labels)):\n",
    "        curr_lab = labels[i]\n",
    "        prev_lab = labels[i-1]\n",
    "        if curr_lab in range(7,14):\n",
    "            if prev_lab != curr_lab and prev_lab != curr_lab - 7:\n",
    "                labels[i] = curr_lab -7\n",
    "    return labels\n",
    "\n",
    "fix_beginnings(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize and add labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    o = tokenizer(examples['text'], truncation=True, padding=True, return_offsets_mapping=True, max_length=max_length, stride=stride, return_overflowing_tokens=True)\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = o[\"overflow_to_sample_mapping\"]\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = o[\"offset_mapping\"]\n",
    "    \n",
    "    o[\"labels\"] = []\n",
    "\n",
    "    for i in range(len(offset_mapping)):\n",
    "                   \n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n",
    "\n",
    "        for label_start, label_end, label in \\\n",
    "        list(zip(examples['starts'][sample_index], examples['ends'][sample_index], examples['classlist'][sample_index])):\n",
    "            for j in range(len(labels)):\n",
    "                token_start = offset_mapping[i][j][0]\n",
    "                token_end = offset_mapping[i][j][1]\n",
    "                if token_start == label_start: \n",
    "                    labels[j] = l2i[f'B-{label}']    \n",
    "                if token_start > label_start and token_end <= label_end: \n",
    "                    labels[j] = l2i[f'I-{label}']\n",
    "\n",
    "        for k, input_id in enumerate(o['input_ids'][i]):\n",
    "            if input_id in [0,1,2]:\n",
    "                labels[k] = -100\n",
    "\n",
    "        labels = fix_beginnings(labels)\n",
    "                   \n",
    "        o[\"labels\"].append(labels)\n",
    "        \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23371859fa14592a4c78488f0079bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6a8c603e724bfe860c043490960ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, \\\n",
    "                                  batch_size=20000, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'labels'],\n",
       "        num_rows: 14574\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'labels'],\n",
       "        num_rows: 1625\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d491ee4a430742f08a569c0c521c594e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForTokenClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# we will use auto model for token classification\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=N_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BS,\n",
    "    per_device_eval_batch_size=BS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WD,\n",
    "    report_to='wandb', \n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    warmup_ratio=WARMUP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b5ccd1025c4a4a847ab3d94079afc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is not the competition metric, but for now this will be better than nothing...\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [i2l[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [i2l[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "***** Running training *****\n",
      "  Num examples = 14574\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 2275\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1345' max='2275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1345/2275 1:19:25 < 55:00, 0.28 it/s, Epoch 2.95/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.911700</td>\n",
       "      <td>0.628397</td>\n",
       "      <td>0.116046</td>\n",
       "      <td>0.252191</td>\n",
       "      <td>0.158951</td>\n",
       "      <td>0.793647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.558838</td>\n",
       "      <td>0.166928</td>\n",
       "      <td>0.315946</td>\n",
       "      <td>0.218443</td>\n",
       "      <td>0.813161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1625\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to longformer-base-4096-finetuned-ner/checkpoint-455\n",
      "Configuration saved in longformer-base-4096-finetuned-ner/checkpoint-455/config.json\n",
      "Model weights saved in longformer-base-4096-finetuned-ner/checkpoint-455/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-base-4096-finetuned-ner/checkpoint-455/tokenizer_config.json\n",
      "Special tokens file saved in longformer-base-4096-finetuned-ner/checkpoint-455/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1625\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to longformer-base-4096-finetuned-ner/checkpoint-910\n",
      "Configuration saved in longformer-base-4096-finetuned-ner/checkpoint-910/config.json\n",
      "Model weights saved in longformer-base-4096-finetuned-ner/checkpoint-910/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-base-4096-finetuned-ner/checkpoint-910/tokenizer_config.json\n",
      "Special tokens file saved in longformer-base-4096-finetuned-ner/checkpoint-910/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_for_validation(examples):\n",
    "\n",
    "    o = tokenizer(examples['text'], truncation=True, return_offsets_mapping=True, max_length=4096)\n",
    "\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = o[\"offset_mapping\"]\n",
    "    \n",
    "    o[\"labels\"] = []\n",
    "\n",
    "    for i in range(len(offset_mapping)):\n",
    "                   \n",
    "        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n",
    "\n",
    "        for label_start, label_end, label in \\\n",
    "        list(zip(examples['starts'][i], examples['ends'][i], examples['classlist'][i])):\n",
    "            for j in range(len(labels)):\n",
    "                token_start = offset_mapping[i][j][0]\n",
    "                token_end = offset_mapping[i][j][1]\n",
    "                if token_start == label_start: \n",
    "                    labels[j] = l2i[f'B-{label}']    \n",
    "                if token_start > label_start and token_end <= label_end: \n",
    "                    labels[j] = l2i[f'I-{label}']\n",
    "\n",
    "        for k, input_id in enumerate(o['input_ids'][i]):\n",
    "            if input_id in [0,1,2]:\n",
    "                labels[k] = -100\n",
    "\n",
    "        labels = fix_beginnings(labels)\n",
    "                   \n",
    "        o[\"labels\"].append(labels)\n",
    "        \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_val = datasets.map(tokenize_for_validation, batched=True)\n",
    "tokenized_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ground truth for validation\n",
    "\n",
    "l = []\n",
    "for example in tokenized_val['test']:\n",
    "    for c, p in list(zip(example['classlist'], example['predictionstrings'])):\n",
    "        l.append({\n",
    "            'id': example['id'],\n",
    "            'discourse_type': c,\n",
    "            'predictionstring': p,\n",
    "        })\n",
    "    \n",
    "gt_df = pd.DataFrame(l)\n",
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualization with displacy\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from pylab import cm, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path('../input/feedback-prize-2021/train')\n",
    "\n",
    "colors = {\n",
    "            'Lead': '#8000ff',\n",
    "            'Position': '#2b7ff6',\n",
    "            'Evidence': '#2adddd',\n",
    "            'Claim': '#80ffb4',\n",
    "            'Concluding Statement': 'd4dd80',\n",
    "            'Counterclaim': '#ff8042',\n",
    "            'Rebuttal': '#ff0000',\n",
    "            'Other': '#007f00',\n",
    "         }\n",
    "\n",
    "def visualize(df, text):\n",
    "    ents = []\n",
    "    example = df['id'].loc[0]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        ents.append({\n",
    "                        'start': int(row['discourse_start']), \n",
    "                         'end': int(row['discourse_end']), \n",
    "                         'label': row['discourse_type']\n",
    "                    })\n",
    "\n",
    "    doc2 = {\n",
    "        \"text\": text,\n",
    "        \"ents\": ents,\n",
    "        \"title\": example\n",
    "    }\n",
    "\n",
    "    options = {\"ents\": train.discourse_type.unique().tolist() + ['Other'], \"colors\": colors}\n",
    "    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_val['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions, axis=-1)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code that will convert our predictions into prediction strings, and visualize it at the same time\n",
    "# this most likely requires some refactoring\n",
    "\n",
    "def get_class(c):\n",
    "    if c == 14: return 'Other'\n",
    "    else: return i2l[c][2:]\n",
    "\n",
    "def pred2span(pred, example, viz=False, test=False):\n",
    "    example_id = example['id']\n",
    "    n_tokens = len(example['input_ids'])\n",
    "    classes = []\n",
    "    all_span = []\n",
    "    for i, c in enumerate(pred.tolist()):\n",
    "        if i == n_tokens-1:\n",
    "            break\n",
    "        if i == 0:\n",
    "            cur_span = example['offset_mapping'][i]\n",
    "            classes.append(get_class(c))\n",
    "        elif i > 0 and (c == pred[i-1] or (c-7) == pred[i-1]):\n",
    "            cur_span[1] = example['offset_mapping'][i][1]\n",
    "        else:\n",
    "            all_span.append(cur_span)\n",
    "            cur_span = example['offset_mapping'][i]\n",
    "            classes.append(get_class(c))\n",
    "    all_span.append(cur_span)\n",
    "    \n",
    "    if test: text = get_test_text(example_id)\n",
    "    else: text = get_raw_text(example_id)\n",
    "    \n",
    "    # abra ka dabra se soli fanta ko pelo\n",
    "    \n",
    "    # map token ids to word (whitespace) token ids\n",
    "    predstrings = []\n",
    "    for span in all_span:\n",
    "        span_start = span[0]\n",
    "        span_end = span[1]\n",
    "        before = text[:span_start]\n",
    "        token_start = len(before.split())\n",
    "        if len(before) == 0: token_start = 0\n",
    "        elif before[-1] != ' ': token_start -= 1\n",
    "        num_tkns = len(text[span_start:span_end+1].split())\n",
    "        tkns = [str(x) for x in range(token_start, token_start+num_tkns)]\n",
    "        predstring = ' '.join(tkns)\n",
    "        predstrings.append(predstring)\n",
    "                    \n",
    "    rows = []\n",
    "    for c, span, predstring in zip(classes, all_span, predstrings):\n",
    "        e = {\n",
    "            'id': example_id,\n",
    "            'discourse_type': c,\n",
    "            'predictionstring': predstring,\n",
    "            'discourse_start': span[0],\n",
    "            'discourse_end': span[1],\n",
    "            'discourse': text[span[0]:span[1]+1]\n",
    "        }\n",
    "        rows.append(e)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['length'] = df['discourse'].apply(lambda t: len(t.split()))\n",
    "    \n",
    "    # short spans are likely to be false positives, we can choose a min number of tokens based on validation\n",
    "    df = df[df.length > min_tokens].reset_index(drop=True)\n",
    "    if viz: visualize(df, text)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred2span(preds[0], tokenized_val['test'][0], viz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred2span(preds[1], tokenized_val['test'][1], viz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i in range(len(tokenized_val['test'])):\n",
    "    dfs.append(pred2span(preds[i], tokenized_val['test'][i]))\n",
    "\n",
    "pred_df = pd.concat(dfs, axis=0)\n",
    "pred_df['class'] = pred_df['discourse_type']\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source: https://www.kaggle.com/robikscube/student-writing-competition-twitch#Competition-Metric-Code\n",
    "\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(\" \"))\n",
    "    set_gt = set(row.predictionstring_gt.split(\" \"))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter / len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp_micro(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "\n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = (\n",
    "        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    pred_df[\"pred_id\"] = pred_df.index\n",
    "    gt_df[\"gt_id\"] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(\n",
    "        gt_df,\n",
    "        left_on=[\"id\", \"class\"],\n",
    "        right_on=[\"id\", \"discourse_type\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_pred\", \"_gt\"),\n",
    "    )\n",
    "    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n",
    "    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n",
    "\n",
    "    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n",
    "    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n",
    "    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n",
    "    tp_pred_ids = (\n",
    "        joined.query(\"potential_TP\")\n",
    "        .sort_values(\"max_overlap\", ascending=False)\n",
    "        .groupby([\"id\", \"predictionstring_gt\"])\n",
    "        .first()[\"pred_id\"]\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n",
    "    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    # calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n",
    "    return my_f1_score\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
    "    class_scores = {}\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n",
    "        pred_subset = (\n",
    "            pred_df.loc[pred_df[\"class\"] == discourse_type]\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n",
    "        class_scores[discourse_type] = class_score\n",
    "    f1 = np.mean([v for v in class_scores.values()])\n",
    "    if return_class_scores:\n",
    "        return f1, class_scores\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_feedback_comp(pred_df, gt_df, return_class_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End\n",
    "\n",
    "I'll appreciate every upvote or comment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
