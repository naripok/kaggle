{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Inference Baseline\n",
    "\n",
    "Training notebook: https://www.kaggle.com/thedrcat/feedback-prize-huggingface-baseline-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "batch_size = 1\n",
    "min_tokens = 5\n",
    "tok_checkpoint = './longformer-base-4096/'\n",
    "model_checkpoint = './longformer-base-4096/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  class  predictionstring\n",
       "0  18409261F5C2    NaN               NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('../../input/feedback-prize-2021/train.csv')\n",
    "train.head(1)\n",
    "\n",
    "test = pd.read_csv('../../input/feedback-prize-2021/sample_submission.csv')\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup dictionaries\n",
    "classes = train.discourse_type.unique().tolist()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "tags = defaultdict()\n",
    "for i, c in enumerate(classes):\n",
    "    tags[f'B-{c}'] = i\n",
    "    tags[f'I-{c}'] = i + len(classes)\n",
    "tags[f'O'] = len(classes) * 2\n",
    "tags[f'Special'] = -100\n",
    "l2i = dict(tags)\n",
    "\n",
    "i2l = defaultdict()\n",
    "for k, v in l2i.items(): \n",
    "    i2l[v] = k\n",
    "i2l[-100] = 'Special'\n",
    "i2l = dict(i2l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "from pathlib import Path\n",
    "\n",
    "test_path = Path('../../input/feedback-prize-2021/test')\n",
    "\n",
    "def get_test_text(ids):\n",
    "    with open(test_path/f'{ids}.txt', 'r') as file: data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(tok_checkpoint, num_labels=len(i2l)-1)\n",
    "\n",
    "model.load_state_dict(torch.load(model_checkpoint))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll use trainer with the loaded model to run inference on test set\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code that will convert our predictions into prediction strings. we'll skip visualization here. \n",
    "# this most likely requires some refactoring\n",
    "\n",
    "def get_class(c):\n",
    "    if c == 14: return 'Other'\n",
    "    else: return i2l[c][2:]\n",
    "\n",
    "def pred2span(pred, example, viz=False, test=False):\n",
    "    example_id = example['id']\n",
    "    n_tokens = len(example['input_ids'])\n",
    "    classes = []\n",
    "    all_span = []\n",
    "    for i, c in enumerate(pred.tolist()):\n",
    "        if i == n_tokens-1:\n",
    "            break\n",
    "        if i == 0:\n",
    "            cur_span = example['offset_mapping'][i]\n",
    "            classes.append(get_class(c))\n",
    "        elif i > 0 and (c == pred[i-1] or (c-7) == pred[i-1]):\n",
    "            cur_span[1] = example['offset_mapping'][i][1]\n",
    "        else:\n",
    "            all_span.append(cur_span)\n",
    "            cur_span = example['offset_mapping'][i]\n",
    "            classes.append(get_class(c))\n",
    "    all_span.append(cur_span)\n",
    "    \n",
    "    if test: text = get_test_text(example_id)\n",
    "    else: text = get_raw_text(example_id)\n",
    "        \n",
    "    # map token ids to word (whitespace) token ids\n",
    "    predstrings = []\n",
    "    for span in all_span:\n",
    "        span_start = span[0]\n",
    "        span_end = span[1]\n",
    "        before = text[:span_start]\n",
    "        token_start = len(before.split())\n",
    "        if len(before) == 0: token_start = 0\n",
    "        elif before[-1] != ' ': token_start -= 1\n",
    "        num_tkns = len(text[span_start:span_end+1].split())\n",
    "        tkns = [str(x) for x in range(token_start, token_start+num_tkns)]\n",
    "        predstring = ' '.join(tkns)\n",
    "        predstrings.append(predstring)\n",
    "                    \n",
    "    rows = []\n",
    "    for c, span, predstring in zip(classes, all_span, predstrings):\n",
    "        e = {\n",
    "            'id': example_id,\n",
    "            'discourse_type': c,\n",
    "            'predictionstring': predstring,\n",
    "            'discourse_start': span[0],\n",
    "            'discourse_end': span[1],\n",
    "            'discourse': text[span[0]:span[1]+1]\n",
    "        }\n",
    "        rows.append(e)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['length'] = df['discourse'].apply(lambda t: len(t.split()))\n",
    "    \n",
    "    # short spans are likely to be false positives, we can choose a min number of tokens based on validation\n",
    "    df = df[df.length > min_tokens].reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>During a group project, have you ever asked a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>80% of Americans believe seeking multiple opin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>When people ask for advice,they sometimes talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Have you ever asked more than one person for h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0FB0700DAF44  During a group project, have you ever asked a ...\n",
       "1  18409261F5C2  80% of Americans believe seeking multiple opin...\n",
       "2  D46BCB48440A  When people ask for advice,they sometimes talk...\n",
       "3  D72CB1C11673  Making choices in life can be very difficult. ...\n",
       "4  DF920E0A7337  Have you ever asked more than one person for h..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "import os \n",
    "\n",
    "files = os.listdir('../../input/feedback-prize-2021/test')\n",
    "ids = [x.split('.')[0] for x in files]\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['id'] = ids\n",
    "df_test['text'] = df_test['id'].apply(get_test_text)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_ds = Dataset.from_pandas(df_test)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_for_test(examples):\n",
    "\n",
    "    o = tokenizer(examples['text'], truncation=True, return_offsets_mapping=True, max_length=4096)\n",
    "  \n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401bf2c11c2a4f06ba980adefb5733fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'input_ids', 'attention_mask', 'offset_mapping'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test = test_ds.map(tokenize_for_test)\n",
    "tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: id, offset_mapping, text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n",
      "Input ids are automatically padded from 1304 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1304, 15), (5, 1304))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions, axis=-1)\n",
    "predictions.shape, preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i in range(len(tokenized_test)):\n",
    "    dfs.append(pred2span(preds[i], tokenized_test[i], test=True))\n",
    "\n",
    "pred_df = pd.concat(dfs, axis=0)\n",
    "pred_df['class'] = pred_df['discourse_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pred_df[['id', 'class', 'predictionstring']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv('../../output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████████████████████████████████| 13.0k/13.0k [00:02<00:00, 5.72kB/s]\n",
      "400 - Bad Request\n"
     ]
    }
   ],
   "source": [
    "# !kaggle competitions submit -c \"feedback-prize-2021\" -f \"../../output/submission.csv\" -m \"huggingface-longformer-baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Starting upload for file tokenizer.json\n",
      "100%|██████████████████████████████████████| 2.01M/2.01M [00:02<00:00, 1.04MB/s]\n",
      "Upload successful: tokenizer.json (2MB)\n",
      "Starting upload for file special_tokens_map.json\n",
      "100%|████████████████████████████████████████████| 239/239 [00:02<00:00, 113B/s]\n",
      "Upload successful: special_tokens_map.json (239B)\n",
      "Starting upload for file pytorch_model.bin\n",
      "100%|████████████████████████████████████████| 565M/565M [00:34<00:00, 17.3MB/s]\n",
      "Upload successful: pytorch_model.bin (565MB)\n",
      "Starting upload for file tokenizer_config.json\n",
      "100%|████████████████████████████████████████████| 368/368 [00:01<00:00, 216B/s]\n",
      "Upload successful: tokenizer_config.json (368B)\n",
      "Starting upload for file training_args.bin\n",
      "100%|██████████████████████████████████████| 2.92k/2.92k [00:01<00:00, 1.51kB/s]\n",
      "Upload successful: training_args.bin (3KB)\n",
      "Starting upload for file vocab.json\n",
      "100%|█████████████████████████████████████████| 780k/780k [00:02<00:00, 339kB/s]\n",
      "Upload successful: vocab.json (780KB)\n",
      "Starting upload for file config.json\n",
      "100%|████████████████████████████████████████| 1.56k/1.56k [00:02<00:00, 537B/s]\n",
      "Upload successful: config.json (2KB)\n",
      "Starting upload for file merges.txt\n",
      "100%|█████████████████████████████████████████| 446k/446k [00:02<00:00, 204kB/s]\n",
      "Upload successful: merges.txt (446KB)\n",
      "Dataset version is being created. Please check progress at https://www.kaggle.com/fernandocanteruccio/fp21lb\n"
     ]
    }
   ],
   "source": [
    "# !kaggle datasets init -p \"./longformer-base-4096\"\n",
    "# !kaggle datasets create -p \"./longformer-base-4096\"\n",
    "!kaggle datasets version -p \"./longformer-base-4096\" -m \"v0.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fileName                                        date                 description                                              status    publicScore  privateScore  \n",
      "----------------------------------------------  -------------------  -------------------------------------------------------  --------  -----------  ------------  \n",
      "Feedback Prize HuggingFace Baseline: Inference  2022-02-07 14:38:19  Notebook Feedback Prize HuggingFace Baseline: Inference  complete  0.610        None          \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions \"feedback-prize-2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
